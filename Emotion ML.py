# -*- coding: utf-8 -*-
"""Emotion_ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z1_etoOKmUjonqIsMcxhHc-ota41TTLQ
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, accuracy_score
from sklearn.feature_extraction.text import TfidfVectorizer

from google.colab import files
import io

uploaded = files.upload()
for file_name in uploaded.keys():
    raw_data = pd.read_csv(io.BytesIO(uploaded[file_name]), delimiter=';')

    print(f"Data from file: {file_name}")
    print(raw_data.head())

data = pd.read_csv('/content/Raw Data.csv', delimiter=';')
data.head(3)

"""**EDA**"""

print(data['Emotion'].unique())

"""**Preprocessing Text Data**

Cleaning the text data by removing unnecessary characters, converting to lowercase, and tokenizing.
"""

import re

def clean_text(text):
    text = re.sub(r'\W', ' ', text)  # Remove special characters
    text = text.lower()               # Convert to lowercase
    text = text.strip()               # Remove leading and trailing spaces
    return text

data['Comment'] = data['Comment'].apply(clean_text)

"""**Split the Data**

"""

X = data['Comment']
y = data['Emotion']

# First, split into training and temp (which will be further split)
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)

# Now split the temp data into validation and test sets
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.6, random_state=42)

print("Training data shape:", X_train.shape)
print("Validation data shape:", X_val.shape)
print("Test data shape:", X_test.shape)

"""**Convert Text to Numeric Data**

Use TF-IDF (Term Frequency-Inverse Document Frequency)
"""

vectorizer = TfidfVectorizer()

X_train_vectorized = vectorizer.fit_transform(X_train)
X_val_vectorized = vectorizer.transform(X_val)
X_test_vectorized = vectorizer.transform(X_test)

"""**Train the Naive Bayes Model**

Initialize and fit the Multinomial Naive Bayes model.
"""

model = MultinomialNB()
model.fit(X_train_vectorized, y_train)

"""**Evaluate and Predict the Model**

Validation Data
"""

y_val_pred = model.predict(X_val_vectorized)

print(f'Validation Accuracy: {accuracy_score(y_val, y_val_pred)}')
print(classification_report(y_val, y_val_pred))

"""Test Data"""

y_test_pred = model.predict(X_test_vectorized)

print(f'Test Accuracy: {accuracy_score(y_test, y_test_pred)}')
print(classification_report(y_test, y_test_pred))

"""**Predicting New Comments**

To predict the emotion of a new comment, preprocess it and use the model.
"""

new_comment = "Whats wrong with you!"
cleaned_comment = clean_text(new_comment)
vectorized_comment = vectorizer.transform([cleaned_comment])
predicted_emotion = model.predict(vectorized_comment)

print(f'The predicted emotion is: {predicted_emotion[0]}')

"""**Exporting the model**

Save the Model and Vectorizer

After training your model, you can save both the model and the vectorizer to disk
"""

import joblib

# Save the model
joblib.dump(model, 'emotion_classifier_model.pkl')

# Save the vectorizer
joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')

"""**Analyze new dataset**"""

from google.colab import files
import io

uploaded = files.upload()
for file_name in uploaded.keys():
    test_data = pd.read_csv(io.BytesIO(uploaded[file_name]), delimiter=',')

new_data = pd.read_csv('/content/Test Data.csv', delimiter=',')
new_data.head(3)

import joblib

# Load the model and vectorizer
model = joblib.load('emotion_classifier_model.pkl')
vectorizer = joblib.load('tfidf_vectorizer.pkl')


# Clean the comments in the new dataset
def clean_text(text):
    text = re.sub(r'\W', ' ', text)
    text = text.lower()
    text = text.strip()
    return text

new_data['Comment'] = new_data['Comment'].apply(clean_text)

# Transform the comments using the loaded vectorizer
new_comments_vectorized = vectorizer.transform(new_data['Comment'])

# Make predictions
predicted_emotions = model.predict(new_comments_vectorized)

# Get the probabilities for each class
predicted_probabilities = model.predict_proba(new_comments_vectorized)

# Combine comments with predicted emotions and probabilities
results = pd.DataFrame({
    'Comment': new_data['Comment'],
    'Predicted Emotion': predicted_emotions,
    'Probability': predicted_probabilities.max(axis=1)  # Get the highest probability for each prediction
})

print(results)

"""**Export the results**"""

# Save results to a CSV file
results.to_csv('predicted_emotions.csv', index=False)

# Download the CSV file
from google.colab import files
files.download('predicted_emotions.csv')

"""**Using BERT modelling**"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
from transformers import BertTokenizer, TFBertForSequenceClassification
import tensorflow as tf

# Step 3: Load and Prepare Your Data

# Split the data into train, validation, and test sets
X_train, X_temp, y_train, y_temp = train_test_split(data['Comment'], data['Emotion'], test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Step 4: Tokenization
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Tokenize the data
def tokenize_data(texts):
    return tokenizer(texts.tolist(), padding=True, truncation=True, return_tensors='tf')

train_encodings = tokenize_data(X_train)
val_encodings = tokenize_data(X_val)
test_encodings = tokenize_data(X_test)

# Step 5: Convert Labels to TensorFlow Format
label_map = {label: idx for idx, label in enumerate(y_train.unique())}
y_train_numeric = y_train.map(label_map)
y_val_numeric = y_val.map(label_map)
y_test_numeric = y_test.map(label_map)

# Create TensorFlow datasets
train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train_numeric)).batch(16)
val_dataset = tf.data.Dataset.from_tensor_slices((dict(val_encodings), y_val_numeric)).batch(16)
test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), y_test_numeric)).batch(16)

# Step 6: Model Training
model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_map))

# Compile the model
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

# Train the model
model.fit(train_dataset, validation_data=val_dataset, epochs=3)

# Step 7: Evaluate the Model
y_pred = model.predict(test_dataset)
y_pred_labels = tf.argmax(y_pred.logits, axis=1)

# Calculate accuracy and classification report
print(f'Test Accuracy: {accuracy_score(y_test_numeric, y_pred_labels)}')
print(classification_report(y_test_numeric, y_pred_labels, target_names=label_map.keys()))