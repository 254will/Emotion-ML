# -*- coding: utf-8 -*-
"""Emotional_ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Nc8oA0Uy9rZW7oNaduMVdUQt_AoZX5aJ

# **Training, Testing and Validating the Model**
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score
from transformers import BertTokenizer, TFBertForSequenceClassification
import tensorflow as tf

from google.colab import files
import io

uploaded = files.upload()
for file_name in uploaded.keys():
    raw_data = pd.read_csv(io.BytesIO(uploaded[file_name]), delimiter=';')

data = pd.read_csv('/content/Raw Data.csv', delimiter=';')
data.head(3)

"""The model"""

# Step 1: Load and Prepare Your Data

# Split the data into train, validation, and test sets
X_train, X_temp, y_train, y_temp = train_test_split(data['Comment'], data['Emotion'], test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Step 2: Tokenization
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

# Tokenize the data
def tokenize_data(texts):
    return tokenizer(texts.tolist(), padding=True, truncation=True, return_tensors='tf')

train_encodings = tokenize_data(X_train)
val_encodings = tokenize_data(X_val)
test_encodings = tokenize_data(X_test)

# Step 3: Convert Labels to TensorFlow Format
label_map = {label: idx for idx, label in enumerate(y_train.unique())}
y_train_numeric = y_train.map(label_map)
y_val_numeric = y_val.map(label_map)
y_test_numeric = y_test.map(label_map)

# Create TensorFlow datasets
train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), y_train_numeric)).batch(32)
val_dataset = tf.data.Dataset.from_tensor_slices((dict(val_encodings), y_val_numeric)).batch(32)
test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_encodings), y_test_numeric)).batch(32)

# Step 4: Model Training
model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_map))

# Compile the model
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

# Train the model
model.fit(train_dataset, validation_data=val_dataset, epochs=2)

# Step 5: Evaluate the Model
y_pred = model.predict(test_dataset)
y_pred_labels = tf.argmax(y_pred.logits, axis=1)

# Calculate accuracy and classification report
print(f'Test Accuracy: {accuracy_score(y_test_numeric, y_pred_labels)}')
print(classification_report(y_test_numeric, y_pred_labels, target_names=label_map.keys()))

"""Save and dowload the model"""

model.save_pretrained('/content/bert_model')
tokenizer.save_pretrained('/content/bert_model')

# Download the saved model files
import shutil

# Zip the model directory
shutil.make_archive('/content/bert_model', 'zip', '/content/bert_model')

# Download the zip file
from google.colab import files
files.download('/content/bert_model.zip')

"""# **Using the Model on New Data**

Import libraries
"""

import torch
from transformers import BertTokenizer, BertForSequenceClassification
from google.colab import files
import zipfile
import os,io

"""Extract the Zip File"""

# Upload the zip file
uploaded = files.upload()

# Assuming the zip file is named 'bert_model.zip'
with zipfile.ZipFile('Bert_Model.zip', 'r') as zip_ref:
    zip_ref.extractall('/content/Bert_Model')

"""Load the Model and Tokenizer"""

# Define the path to the extracted model directory
model_dir = '/content/Bert_Model'

# Load the model and tokenizer
tokenizer = BertTokenizer.from_pretrained(model_dir)
model = BertForSequenceClassification.from_pretrained(model_dir)

"""Load the dataset"""

uploaded = files.upload()
for file_name in uploaded.keys():
    data = pd.read_csv(io.BytesIO(uploaded[file_name]), delimiter=';')
    data.head(3)

comments = data['Comment']  # Adjust this to match your column name

"""Tokenize the Comments"""

inputs = tokenizer(comments.tolist(), padding=True, truncation=True, return_tensors='pt')

"""Make Predictions"""

# Set the model to evaluation mode
model.eval()

# Make predictions
with torch.no_grad():
    outputs = model(**inputs)
    logits = outputs.logits  # Get the raw predictions

# Calculate probabilities
probabilities = torch.softmax(logits, dim=1)  # Convert logits to probabilities

"""Interpret Predictions"""

# Get the predicted class
predicted_classes = torch.argmax(probabilities, dim=1)

# Assuming you have a mapping of class indices to emotions
emotion_mapping = {0: 'Emotion1', 1: 'Emotion2', 2: 'Emotion3'}  # Update with your actual mappings
predicted_emotions = [emotion_mapping[idx.item()] for idx in predicted_classes]

# Combine results into a DataFrame
results = pd.DataFrame({
    'Comment': comments,
    'Predicted Emotion': predicted_emotions,
    'Probability': [prob.max().item() for prob in probabilities]
})

# Display results
print(results)

"""Save in csv format"""

# Save results to a CSV file
results.to_csv('predicted_emotions.csv', index=False)

# Download the CSV file
from google.colab import files
files.download('predicted_emotions.csv')